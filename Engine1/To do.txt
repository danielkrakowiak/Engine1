19.03.2014
	- when creating fipImage in Texture2D - FREE_IMAGE_TYPE is set to FIT_BITMAP by default. However, it is not always the appropriate file type for image. Ex.: 16 bit grayscale heighmaps have different file type. Maybe it should depend on special file formats defined in AssetFileFormat such as RAW_16BIT?
28.03.2014
	- Handle errors in AssetManager::loadAssetsFromDisk and AssetManager::loadAssets - threads shoould not be terminated, exceptions should not be thrown, path to assets should be removed from assets - maybe handle by not doing anything?
11.04.2014
	- Write = operator for every class with copy constructor (otherwise default copy constructor will be used)
12.04.2014
	- check whether all DirectX headers point to the right SDK (by pointing the cursor over an include)\
	- add loading to Gpu to asset manager (read about async resource loading before - probably one loading thread is enough)
08.11.2014
	- add more methods for manipulation float44 and float43 - ex. getRotation rename to getRotation, get4x3 rename to getRotationTranslation, add setRotationRow1 etc
	- change bone pose format from float44 to float43 - beacuse last column is useless anyway
15.11.2014
	- methods which return references (especially to lists) should retrurn shared pointer rather than reference. 
	- add File class with path, type, data, loadedFlag? use it in all assets classes (Mesh, Texture etc)
	- SkeletonAnimation requires bonesNames to work (mapping of names to bone indexes) - how should these names be passed so that each SkeletonAnimation doesn't store a copy of that list (shared_ptr?)
	- should boneNames have it's own class with methods like getBoneIndex( std::string name ) ? - would it be used anywhere else but loading animations?
21.11.2014
	- does combining two SkeletonPoses really work? Especially in the case where the same bones are manipulated in both poses. How about joining three or more poses?
	- optimizing SkeletonPose in terms of memory usage? What is the size of this object when used for a typical or large animation?
28.12.2014
	- Const methods ( ex. const Mesh & getMesh() const ) should always return const-object or const-reference. Non-const versions of the same methods can be created to allow modification of the object. 
	- add const and non-const version of some methods (getters mostly) to all classes. Use const method in non-const method. Use const_cast<type> to convert types from const to non-const.
13.07.2015
	- Go through all "throw std::exception" and make sure that throwing method has parenthesis "()" in the error message.
	- Add tests for SkeletonMeshVertexShader checking if it throws for incorrect bones-per-vertex-values.
11.11.2015
	-  Texture2D::createFromMemory - WARNING: casting away const qualifier on data - need to make sure that FreeImage doesn't modify the input data!
12.11.2015
	- AssetManager: add due time as priority for each asset in AssetManager. Asset's subassets should be loaded with it's owner's priority.
	  When inserting assets to load in the queues they need to be inserted in the order of increasing due time.
	- IMPORTANT: There is one problem: if one thread (loadAssets method) waits for the sub-assets to be loaded, then it's not loading any assets at that time. Maybe there should be more threads, but only the given number of them active? Non-binary mutex or something?
29.04.2016
	- [Texture] Add constructor from exisitng DX texture and render target view (or swap chain?). For access to the backbuffer.
	- [Texture - Refactor] Can I remove initialize methods from Texture2DGeneric? Move that code to constuctors...
	- [Includes Cleanup] Some headers include <d3d11.h> although they only contain references and pointers to DX types. Replace with forward declarations.
	- [Test Texture] I should test whether constructor options like generateMipMaps (ex. for RenderTarget) really work for all the bindings and remove them if they are not supported.
21.08.2016
	- [Texture - new] How to handle texture arrays (also with mipmaps). Should it be a separate class. How to handle all that complexity? How to use the existing texture class?
	- [Shader params] Add support for setting empty SRVs, UAVs etc among non-empty ones.
31.08.2016
	- [ Texture copy ] When copying texture GPU - GPU we should check if their dimensions are the same - otherwise throw exception.
20.09.2016
	- [ BinaryFile ] Reading/Writing could a template method - able to read any type from file - such as int/float/float3 - it works as a copy anyways...
25.01.2016
	- [RendererCore - refactoring] RendererCore::copyTexture method is a template, so any texture works fine when passed. 
	  But compiler cannot deduce template parameter type and perform implicit type conversions at the same type, so it's confused and fails to compile.
	  How to avoid having to explicitly cast textures to avoid compiler confusion? Add some methods for the most common texture types and use template version inside?
03.02.2017
	- [ Path ] Replace all std::string paths with std::filesystem::path - it's internally stored as std::wstring, but allows for many useful operations - such as extracting filename etc.
07.03.2017
	- [ Optimization ] Disabling rendering/compute pipelines could be easily delayed until a different pipeline is needed. Simply remove "disable" method and disable when enabling (if needed).
	  What problems could it cause? Some problems when configuring pipelines? I could enfore that a given pipeline type is enabled by exceptions.

For later ######################################################################################################################

//////////
 - Add some math functions to measure corruption of matrices

 - Add: matrix difference (sum of squared distances), matrix inverse, measure matrix orgonality (multiply inverse by the transpose and return difference to identity matrix)
 - Optionally add: orthogonalization - later on... To fix corrupted matrices and compare visual results

/////////

- option to read os some range of keyframes/time/ticks from animation file
- option to split an animation into a few by keyframe/time/tick etc.

- add move constructor to some classes? (but only the ones which are designed to be returned from methods by object) 
- add move assignment to some classes? (but only to those for which creating another object from an existing one makes sense)

- test combining poses (skeleton-space, parent-space)
- add method to substract poses (leave only the difference between poses)


- add option to ignore bones which are not in mesh when loading an animation file. How to make sure that missing bones are not in between of the existing ones? Useful to apply complex animation to a sipler version of mesh (LOD etc)

- Refactor Font and FontCharacter classes.

- in BlockMesh, SkeletonMesh buffer getters should return reference?

- 3d text rendering

- make RectangleMesh derive from BlockMesh (to be able to render it with BlockMeshShader) ?

Next2:

	- zdublowac gettery w SkeletonMesh tab by byly wersje const i non-const. Jak w BlockMesh.

##################################################################################################################################

--------------------------------------
	Problems with texture class:
	- No generic class without template params to be used as method arguments.
	- PixelType has no usage for textures which are only stored on gpu (like render target) - but restricts their usage in methods etc.

	How to deal with textures which are not loaded, but store file information (when parsing ModelTexture files etc). 
	Should it ba possible to create empty textures? But lock them internally to make any action impossible?
	But probably a more elegant solution can be found. 
-------------------------------------
	IMPORTANT: AssetManager tests (event the first one) make whole test execution to fail for all tests. But only in Release builds. It works fine in Debug. Why?
-------------------------------------

##################################################################################################################################

 - IMPORTANT: ray tracing shader has better performance with D3D10_SHADER_SKIP_OPTIMIZATION flag set. Why? 
   How to configure the shader to avoid optimizations only where it really improves performance, but not everywhere?
   
   Also2: pass stack depth to the shader as constant or...
   Also3: Compile a few versions of the shader with different stack depth and use the appropriate one?

   - After creating BVH for a mesh - mesh's triangle vector could be replaced with the one from BVH. It's just a matter of reordering the triangles.
     Then, there would be no need to send a seperate buffer to GPU just to mark which triangles are in BVH nodes. Nodes would already contain triangles from mesh triangle list.

	 - textures which use float4 for vectors (rayDir, rayOrigin etc) could use float3, because such format exists.

	 - add loading (drag&drop) animations and applying it to the default skeleton model if they match.
	   Would be good to check if they still work and check how reflections look for them.

	- PERFORMANCE: buffer containing "surface position" in DeferredRenderer could be reused to store ray origins in the RaytraceRenderer. 
	    It would have to be read and rewritten as UAV to slithly adjust ray origins along the surface normal to avoid self-collisions of ray with the reflective surface.

	- check how reflection looks when calculated only for even pixels. middle pixels can receive half of the calculated color from each side (addition).
	  What's the performance difference?

	- algorithm creating BVH uses the same step when moving dividing planes no matter the size of the model. 
	That's not too goood. It should use something like 1/1000 of longest bbox dimension or similar.

	- add g-buffer name display in the corner - how to do it elegantly? These are not full rgba textures...

	- final blurring of reflections: sample mipmap one/two levels less blurry than you need (with/without filtering? any cost? use filtering between points) 
	  and then apply 5x5 gaussian blurr to smoothen pixel borders. Or similar.
	  Must read before!!! : https://bartwronski.com/2014/03/23/gdc-follow-up-screenspace-reflections-filtering-and-up-sampling/

	  - should measure the cost of generating mipmaps for the color target! Compare it with cost of running blur kernels.

	  - the problem is bleeding reflections through the edges - note it down somewhere.

	  - create a edge-detection shader which writes distance to the nearest "reflection edge" based on position/normal/ray direction whatever.. Could use multiple passes and shaders.

	  - probably need to add some memory barriers in the compute shader for edge distance. Pixels written by some threads are read by others. Read aboubt sync in DX book.

	  - IMPORTANT: optimization of edge distance calculation: pass "current render pass index" to shader and then ignore threads with current edge distance smaller than pass index 
	    (they are calculated already - no point in recalculating it again). This way less and less pixels will have to be calculated each pass. And zero at some point!


	  - have to upscale mipmaps to at least half the screen size to avoid aliasing when moving camera.

	  - I could limit reflection mipmaps to let's say 4x4 - there is not much point in having even rougher reflections - and to store their upscaled mipmaps at half screen resolution.

	  - upscaling has to happen in steps - upscale by factor of 2 each render pass. Otherwise it gives no advantage over simple sampling. How to do that efficently with 3d texture?

	  - instead of texture array for reflection upscaled mipmaps use 3d texture. It allows for sampling between layers based on dynamic, float level.

	  ////////////////////

	  - why unsigned char textures are displayed/loaded with artifacts? Add texture path to debug name.

	  - IMPORTANT: FreeImage has some trouble loading grayscale png image. What's the problem?

	  - add modifying light params using keyboard shortcuts.

	  Test this idea:

	  - try decreasing far neighbors weigth and check how it influences image. It mimics how most rays reflect near center ray, so they have bigger impact.


	  * To allow for recursive ray tracing:
	  - accumulate specular term multiplier (something like combined surface param)
	  - add total distance covered by ray (maybe except for primary ray depth)
	  - accumulate roughness? accumulate based on distances between levels?

	  How to generate upscaled mipmaps:
	  - start with 2 level mipmap - render it stretched to 3d texture layer 0.
	  - take level 3 mipmap and render it to level 2 mipmap. Then render level 2 mipmap to 3d texture layer 1.
	  - take level 4 mipmap and render it to level 3 mipmap. Then render it to level 2 mipmap. Then to 3d texture layer 2.
	     and so on... - but how about the problem of binding the same resource on input and output? Should I have separate shader resource view for each mipmap level as well to avoid the problem?

   ----------------------------------------------------
   Refactoring:

	- Change naming convention: use m_ in class members.
	- use ComPtr wherever possible instead of raw pointers.
	- use short "Idx" for "index" in the whole code + shaders.
	- rename Texture2D to Texture2D

	- add uint type, which would be just a typedef for unsigned int - could be in math in separate header file.
	- rename "ReflectedRefracted..." shader names to "SecondaryRays...".

	- shader could use g_ in global variable names. Because names often reapeat.

	- get rid of "Direct3D" from class names. It's used all over the code anyway - no reason to mark specific lass with such prefix.

	- all UAVs and render target could be uniformly called "RenderTargets" - everyone knows what that means. No need to differentiate all the time between UAV and RT.

	- should I also const things inside the vectors -> const std::vector< const std::shared_ptr< const Light > > ??

	- rendererCore should also allow for setting viewport dimensions and depth range. It's done manually in FrameRenderer and CombiningRenderer, but it's wrong this way.
   ---------------------------------------------------

*** IMPORTANT:
It's strange that we set textures in FragmentShader's setParameters method, but we set buffers in RendererCore draw method. It should be unified.
Example: draw(or renamed to render) method should take model instead of mesh and set textures accordingly. Or buffers should be set in VertexShader setParameters method...
Also constant buffer is set in setParameters...
Also only the shader know on which slots it binds resources. So only the shader know ho to bind/unbind them correctly. Doing it from outside is just guessing and won't work for more complex/diverse shaders.
So probably set buffers, textures, uavs from shader class.

**** IMPORTANT 2:
Maybe I should createa Buffer class. Same as Texture2D. Because we need to store quite many resource ptr to directx with each buffer plus C++ vector. Could be good to group them up. 
Could be a template class taking it's element type at template argument. It would also simplify sending data to and from GPU using buffers. When building volume data structures etc.
+ Check bind flags on different resources - ex. vertex buffers (and normals etc) have bind flag - "default", but should rather hvae "immutable". Default means that I expect GPU to modify these buffers.


-----------------------------------------

Ability to select an actor by clicking on it. Using ray tracing.

Add other phases of rendering. Rendering color, normals etc. Then rendering the final image. 

How to do raytracing on the GPU --------------------------

First shader: For all rays, find the intersection distance, hit actor (or model), hit triangle -> save to texture.
For each actor in the current scene node run second shader (pass model textures etc):
Second shader: For all rays, calculate the received color.

How to pass many meshes to shader at the same time?
* Maybe use one huge buffer for vertex data and each mesh has a view to only a part of this buffer.
And then also one special view to access the whole buffer.
* Or the same huge buffer approach but with each mesh remebering it's start/end index in that buffer.

----------------------------------------------------------

Implementation order:
1) Generating rays from camera (for test) or from depth buffer
2) Outputing rays as pixels, (pos, albedo, spec, roughness etc)
3) Ray-Box test
4) Shading a layer (no shadows)
5) Shadow-rays tests
6) Storing scene (mesh list/ptrs) on GPU
7) Ray-Mesh test


1) for each layer:
- render depth/pos + albedo/normal/spec/roughness (using DX or RT)
- shade - 1 pass for each light - use SM/RT -> color
2) for each layer - from the last one
- blur the reflections, refractions
- combine with the previous layer

Idea:
- when calculatin range of a light source - it's bounding volume doesn't have to be a sphere. The more precise the bounding volume to more calculations we save. Maybe even a few volumes could describe a lights range?

----------------------------

	  - should I rename refration into transmission? To differentiate them more..

	  - we probably need to store current ray refraction index - the whole history for each ray - as many buffers as there is for reflection/transmission term.
	  - if we hit surface from inside (deduced from normal), than we need to invert refraction index? How to know refractive index of air, water if we are in one mesh, inside another and another etc?

	  - rename albedo render targets to albedoAlpha.

	  - rename reflectionTerm texture in combining shader and classes. Should be called contributionTerm. Check for incorrect assumption regarding reflection/refraction too.

	  - experiment with increasing minimal contribtuion for the ray to be generated and traced. How high can we go for higher levels?

	  

	  - add exceptions and check if wrong level is passed to reflectionrefraction renderer.
	  - remove unnecessary 'level' and 'prev contribution term' from firstreflectionshadindg and firstrefractionshading.
	  
	  -----------------------------

	  - PERFORMANCE IDEA: higher level reflections/refractions could be calculated with smaller work groups - because only some pixels in the group need to be traced and many are black. Large groups cause waste of cores.

	  - MINOR: do we really care about solid object having reflections from backfacing triangles? Like inside a glass wall or aquarium wall? How could we ignore it elegantly with customization?

	  - shading is incorrect for reflection (possibly refraction too). 
	  When something has white albedo and high metalness, I should not see white albedo in reflection but only the reflected color. At least I think so...
	  Commenting out specular part in reflection shading fixes the problem. But what should I really do? Or it actually works when roughness is really low.

	  - should alpha impact the surface color used for shading? Yes. Should it also impact shading reflections?

	  - NEXT: dealing with varying refraction index.
	  - when we enter a surface, we read refractionIndex from screen buffer.
	  - when we leave a surface, we read refractionIndex from our last refraction index (stored in the buffer?)
		- maybe use texture array with refraction indices (like a stack) and the other texture as a counter of where we are on that stack for each ray.
		- when entering a surface - push to stack, when leaving - pop from stack (by decreasing the counter).
		- what format to use for refraction index
		- only need to access these textures when generating secondary rays.
		- store these textures in raytracer like other textures.
		- where to pass/store initial refractive index? Common for all primary rays.

	  - why mipmaps are not generated on gpu in TTexture? Because that would require them to be RenderTargets... There is already a method for that in TTExtureSpecBind. Should we wait for CPU implemantation of generating mipmaps?

	  - IMPORTANT:  In generate refracted rays shaders - Why HLSL compiler crashes without "skip optimization" flag?

	  - PROBLEM: If we don't enter an object, we don't know what's the refractive index outside of that object... So if we start inside of a glass ball, we don't know which IOR to use when leaving the ball.

	  - IMPORTANT: Investigate whether we need to invert normals during normal shading (for backfacing trangles).

	  - IMPORTANT: Pass refraction index for current and prev object when shading refractions (in refraction shading shader).

	  - PERFORMANCE: Maybe it's faster and still correct to invert normals (when ray hits from backface) in raytracing shader rather than in each other shader separately.

	  IMPORTANT:

	  - we assume that refractions are always computed through shaders/classes with "fisrt" in their name. But that may not be true. And the name is misleading. 
	  Because it only happens like that if refraction occurs first, and then reflection/refraction. But if reflection is fisrt, then these shaders/classes are not used at all. What kind of bugs can result from this.
	  Rename shader and classes and replace word "first" with something else.

	  - support for any resolution. - almost done. Something wrong with higher level reflections/refractions. Maybe the contribution term fill size etc? And there is no refractions...
	  - reflection/refraction shading renderers also have some buffers which should have lower resolution. Account for that and downsize them to the same size as raytracer's buffers.
	  - should reflection/refraction shading use integer texcoords? As it's input the same size as output always?

	  - suport for any resolution - part 2 - lower resolution should be accounted for correctly in combining shader to avoid increasing roughness globally (as it happens right now).

	  - raytracing shadows can be done at half resolution (or much less for reflections). And then sampled with interpolation.

	  - how to make skybox not receive shadow? To improve performance...

	  NOW:

	  - when I save distance to occluder in shadow map i take distance to the closest-to-light occluder.
	    But when I do this for ray tracing i get get closest-to-ray origin occluder.

	  - IMPORTANT: what happens with soft shadow when two objects cast shadow which are near to each other, but with different blur radius? On is rather sharp and the other rather soft. How the algorithm should blur the space betweeen them (or covered in both shadows)?


	  - it seems it fucked up something when passing SRVs to raytracing shadows shader.... Fix it first using debug layer...

	  - distance to occluder in shadow rasterize shader should be linearized I guess...

	  - be careful about linearizing depth values when writing distance to occluder - zNear and zFar should be passed as param and should be the same as used to render the shadow maps.

	  - try to compile shader with fxc.exe and load the compiled shader....

	  - shader need to have full names.. because they will be stored in a single output folder.

	  //////////////////////


	   Maybe I can somehow glue buffers together???????? Send them as one to shader despite being many...

	   Higher shader profile???

	  /////////////////////

	  OPTIMIZATION: Calculate pixel position from depth - at least for first reflection/refraction... How about further levels?

	  IDEA: Geometrically incorrect shadow:

	  We create soft shadow within the hard shadow. This way we always know the blur radius. Problem: soft shadow at the edge of hard shadow is half dim. We have to account for that later on.

	  I should also probably use point sampling in bluring phase? Otherwise there is always some mixing of soft/hard shadows for near samples. Even if I reject some of them...

	  Add rejecting samples with wrong normal or depth? Both or just one criteria?

	  I can increase shadow area (to correct soft shadows going only to the inside) by modifying fov, shadow ray direction etc... So it doesn't have to be completaly fake... :)

	  Making edges brighter will fail for shadows from semi-transparent surfaces... How to handle that?

	  //////////////////

	  CURRENT SHADOW PROBLEMS:

	  /////////////////////////

	  DEPTH PEEELING - fix for calculating dist-to-occluder in shadow mapping:

	  - render first shadow map normally - like it is now - the closest object to light is visibile in SM.
	  - render second shadow map, but only the pixels with greater depth than the prev SM. This SM will have all shadowed pixels...

	  - WAAARNING: Depth peeling may also fail? Because it will still get the closest-to-light-dist, but one greater than the closest. And we need the furthest-from-light but closer than some surface... 
	  How to achieve that? Inverted depth test with some maximum? How to know the maximum easily? Read it from screen?

	  - By comparing the distance between first and second SM we know the dist-to-occluder.

	  //////////////////

	  - check which part os lambo has problems? Test1: make everything white, disable shadows and check if shading is ok for everything.

	  - OPTIMIZATION: I probably don't need float for illumination-blur-radius. A BYTE should be enough? We also don't care about fractional parts...

	  - REFERENCES: Screen space soft shadows - described in GPU Pro 6 !!!!!!!!!! Check it!!! 

	  - there should be a second version of RasterizeShadows shader, which takes prev layer ray origin as camera position in each pixel. To handle correct blur radius in reflections.

	  - IMPORTANT MAJOR FIX: I could fix most of the near-screen-edge problems by rendering the whole thing with a bit larger resolution... 
	    I could even lower the quality a bit for those regions. A lot of possible ways and not too much of overhead... I could also ignore those extra regions when performing blurs etc.

	  - make sure to avoid raytracing shadow rays outside of spotlight cone... don't depend on preillumination as it may be falsly lit.

	  - point lights are not working correctly - I need to support them through raytracing everything. Without sampling preillumination etc..

	  - IMPORTANT PROBLEM: when writing distance to occluder in rasterizing shadow map step - we always save it for the closest object... But when two objects cast shadow... the distance to occluder should be saved for the one which gives the most sharp shadow..
	  

	  - OPTIMIZATION: When I read from raw buffers in raytracing shaders I can read multiple values at once. Using Load2, Load3, Load4 methods... I need to try it!!

	  - check all shader with SKIP_OPTIMIZATION flags and check if they are faster/slower with/without this flag.

	  - VERY IMPORTANT: should I switch to reconstructing position from depth globally - it would reduce read bandwidth by factor of 4... And it seems to be the bottleneck...
	    But I need to know ray direction anyways...
	    But maybe later once I clean up the mess with shadows...

	  - why shadowRaytracing shader costs 2ms even if it does nothing, but color the regions for which rays will be traced... Because of running it for every mesh... How ti improve that?
	    I am probably burning bandwidth like hell by reading the same textures (screen-size) over and over..
		Or maybe the problem is the number of runs or the fact that the shader writes to the illumination buffer... Maybe it shouldn't write if does nothing...
		- Idea: I could handle the first copying from preillumination texture as a separate pass... to avoid worrying about it later on for every run.
		- Better idea: Just copying values from preillumination texture. Or preillumination could be copied entirely using a copy drawcall (no need for another shader).
		- Reading the "ray origin" texture seems to be the bottleneck. Probably because it's float4 expensive format. I could use depth instead to save some bandwidth. But it's still not scalable enough...

	  - OPTIMIZATION: Can I deduce for which screen region it is worth to run shadow raytracing shaders by projecting mesh's bounding box on screen - to avoid running shader for pixel which won't do anything in that particular run.
	    To disable whole groups of threads by running them only in some square region of screen.

	  - OPTIMIZATION: when detecting pixels for which its useful to trace shadow rays I detect the need for pixels at the edge of spot light cone. There is no point to trace these rays. How to avoid that? Calculate dot in the shader for that light and compare with cone angle?

	  - use normal when calculating blur radius for shadow... to avoid dim shadow at flat angles.

	  - need to handle casting shadow rays for pixel which are too far from the spot light (ex. above 100m which is max for shadow map).

	  - PROFILING: how to track the total number of traced rays?

	  - Should I use two-pass blurring for soft shadows? May fail, because blur kernel is assymetrical and variable across the screen (soft, hard shadows).

	  NEEDED TEST SCENES:
		- a tower in the desert - to show shadow far from the occluder. How it looks with shadow map and how with ray tracing. How soft shadow acts with distance, how soft shadow look using RT or SM.

		- how to handle semi-transparent models in shadow mapping phase?
		- should there be a separate shadow raytracing shader for lights without shadow maps?

	  - THOUGHTS ON MY TECHNIQUE FOR SOFT SHADOWS:
		- the only way to get sharp shadows is to calculate them in screen-space
		- and when doing it in screen-space I found a prety good technique to make them soft where needed
		- we can also know the needed quality in advance and trace rays where sharp shadow is needed and use shadow map where soft shadow is needed (as optimization)
		- and generally use shadow map to avoid tracing most rays (fully illuminated or shadowed).
		- FIX: I have to fix fake bright samples...
		- FIX: problem when looking at flat angle - smaller screen space is covered in shadow and shadow gets too dim...
		- FIX: handle samples outside of screen - or rather mipmap generation fails for screen regions near egdes..
		- how this blurring would work in reflections? When it is needed?

	  - what is the size of the penumbra? Source: https://www.nde-ed.org/GeneralResources/Formula/RTFormula/Unsharpness/GeometricUnsharpness.htm
	    How much to blur shadows based on light source size, distance from light ot occluder and distance from occluder to shadow receiver.
		u = lightRadius * ( occluderToSurfaceDist / lightToOccluderDist)

	  - Good paper (a bit old now...) describing various soft-shadow techniques: http://maverick.inria.fr/Publications/2003/HLHS03a/SurveyRTSoftShadows.pdf

	  - use sampler2DShadow as in http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/ to know if we are on the shadow border...?

	  - optimization to shadow mapping - use border value and D3D11_TEXTURE_ADDRESS_BORDER and remove if in the shader checking if sample is within texture area.

	  - OPTIMIZATION: make sure to check for max ray distance when intersecting rays with Bounding boxes in view raytracing. As it's done in shadows...

	  - maybe use different vertex shader for shadow maps....

	  - IMPORTANT TEST: How much the shadows would cost if they only used bounindg boxes for tests... Can disable self-shadowing for test..
	  Because if the cost is very low, then maybe it's not a big optimization to use shadow mapping for hard shadows..?

	  - display GPU name and optionally also CPU name on the screen

	  BETTER IDEA: Start merging using with the type (albedo, roughness) which has the most unqiue textures. Then use that texture placement for other texture types.
	  If some merging is not possible (using texcoords, not pixel values) then throw exception.

	  Is my sorting stable? Not moving items which are in correct order?

	  - NOW: UVs get recalculated incorrectly after merge? All meshes have the same UVs ? Or maybe even zeros as UVs?

	  - fix merging models when all models share one alpha but have different other texutres.


	  - check which shaders are running with "skip optimization" flag? Maybe we slow down because of that... Is raytracing shader still faster with that flag?


	  - why skybox has reflections? Why roughness == 1.0f doesn't cancel raytracing? Bad skybox texture?

	  - mipmapping (someone will probably ask about that) ?

	  LATER:

	  - avoid loading models several times - once for each index in file. Especially avoid loading .blockmesh file twice.

	  - all file parsers should:
		- append data to given buffer when writing (not clearing it as it is now!)
		- read data while checking not to cross the given end iterator - should also check for negative sizes etc.
		- how to elegantly avoid resizing data vectors during write (ex: mesh + bvh). - probably the same way as in BVHTreeParser

	  - do I have to account for window frame height, borders etc when creating window? Is my image streched down?

	  - support for variable resolution per reflection/refraction depth

	  - OPTIMIZATION: Replace float4 with float3 wherever possible in framebuffers/shaders.

	  - OPTIMIZATION: Trace 4 or 8 rays in a single shader. Could give amazing boost, because we construct triangle only once and do a few intersection tests with it. Could be tested in normal raytracing or shadow ray tracing (even soft shadows).

	  - FAILED OPTIMIZATION: - pass 4 meshes at once to shadow raytracing shader.

	  - OPTIMIZATION: Mapping/Unmapping constant buffer each time we setParameters to shaders has to be very costly. How can I avoid that?

	  - getting mouse pos is bugged - get absolute positon, while should get position relative to window.

	  - all paths in models/scene should be relative to main folder or asset folder (could be useful if someone else was trying to test my application)
	  
	  - mipmapping
	  - cubemap texture class - to use for skybox

	  MUCH LATER:

	  - I could implement a GPU path tracer to be able to compare normal renderer with physically perfect renderer. That could help a lot in my pursuit of great quality.

	  - BIG PROBLEM: When merging models - we create a texture atlas for albedo textures. But rougness textures or emissive may have completely different dimensions and be placed differently in the atlas.
	  So we somehow have to enforce layout withing the atlas based on the first created atlas. How to deal with big textures?

	  - Support for multiple UV sets when merging models? Some textures use one UV set, others use a different one.

	  - ideas to optimize blurring using shared memory: http://shiba.hpe.cn/jiaoyanzu/WULI/%E6%96%87%E4%BB%B6/soft/directX/Introduction_to_3D_Game_Programming_with_Directx_12.pdf
	    page 721


	    --------------
		BLOOM TODO:
		- split blur into 2-passes and increase number of taps
		- rename blur method and remove radius param.

	    -----------------------------

		###############


		- is initial hard/soft shadow texture in raytracing shadows filled with preShadow?

		###############

	BUGS:
	- there seems to be a bug in reflections hsading - it uses illumination from sorrounding pixels even if they are far away rom the center pixel. 
	  This causes bright glow around lit objects in the normally dark background.


	  FINAL TODO: ---------------------------------------------------------------

	  - add replacing blur radius at the end of raytracing to 0 for all untouched pixels.

	  - [LOOKS DONE] raytracing should store dist-to-occluder nearest to shaded surface - 
	  - check if dist-to-ocluder is same for Shadow Mapping and RT - obviously it's not...
	  - make sure blur-radius is ok in shadowed areas for Shadow Mapping - there are weird problems at shadow intersections between near and far objects.
	  - why rasterizing shadow shader reads previous blur radius value? Is there anyvalue there before that shader is run? What's the point?
	  - IDEA: Sample shadow map with bilinear filtering to get dist-to-occluder - that should make transitions between big pixels smoother. But be careful when to use or not bilinear sampling.
	  - HOPE: With good filtering of blur radius, good blur radius from ray tracing on edges it could be feasible...
	  - IMPORTANT: Set initial blur radius (when clearing texture) to 0, not 1000. It improves filtering, mipmaping etc. 
	    Also sample blur radius at level 4,5 with bilinear filtering. It gives good results for Shadow Mapping.
		Or replace pixels with initial (1000) blur radius at the end, before usign to 0.

	  - some nice antialiasing of shadow could be achieved by limiting minimal blur value to let's say 1 or 2.
	  - Add some antialiasing?

	  - TO TEST: When bluring shadows - use a shadow sample if it blur-radius is similar to shadow sample blur-radius. Requires more reads, but may improve shadow mixing.

	  IDEA: Have 2 textures. In one, spread minimal blur-radius samples. In other, spread maximal blur-radius samples. Then, join both textures?

	  REFACTOR: I could have utulity renderer which does some operations on textures - such as min, max, spread min/max?

	  PERF/QUALITY: Limit screen space or world space shadow blur? And than use that limit to reduce the number of passes required to spread values 
	  (knowing that values are decresing while being spread). If possible...

	  - Spreading soft shadow through hard shadow works pretty well, but it's a bit too much. There is this dark glow around hard shadows...
	    How to reduce it? Faster decresing blur-radius values with number of spreading passes?

		IDEA: Write shadows to two textures - one for soft shadows and one for hard shadows. Each shadow is in fact saved to both textures, lerped using blur-radius.
		Then blur -radius for soft shadows gets spread. Then it gets combined with hard shadows blur-radius (or not?).
		Then when sampling both textures are read an dalso lerped using blur-radius.

		- TODO: Sharper shadow has to spread as well... Otherwise we get too hard edgers for shadows...

		- TODO: normalize the amount of shadow written and read from soft/hard illumination textures. Use equations from the note...

		GOOD IDEAS FOR SPREADING BLUR RADIUS:

		***** USED: *********************

		- generate mipmap for "blur radius" taking avarage only from non-empty samples.
		- spread "blur radius" with avaraging of all non-empty pixels in some radius - it provides first blur of sharp egdes.
		- first spread passes should spread 4px away to ensure that later "smart spread" will work.
		- spread using 3rd mipmap, but than create the 4th mipmap and sample 4th. Because spreading always leaves some sharp egdes, that will smooth them.

		*********************************

		DAILY TO-DO:
		- render which samples get accepted during blur-shadows
		- perspective correction of kernels
		- working on rejecting, weighting samples in both dist-to-occluder search and illumination blurring.
		- adding normal threshold? or ray direction threshold?
		- why dist-to-occluder search is no tdone in two passes?


		IDEA: I could do a couple of passes of dist-to-occluder search, each with a bit larger search radius, but only for empty pixels. This could ensure smaller blur close to original edges.
		Cons: Expensive, because we do all that search for pixels which are too far...

		IDEA: During dist-to-occluder search weight down samples which are farther from center in screen-space and have low dist-to-occluder (sharp shadows away from center).
		---------------------------------

		USEFUL NOTES FOR DIST-TO-OCCLUDER SEARCH: -----------------------
		- using small search area better preserves sharp shadows, but dissallows big, soft shadows
		- using large search area blurs sharp shadows entirely if softer shadow is nearby.

		

		older:
		- be carefull about samples pattern. Take most samples near the center, less further away. 
		  Try to keep circular shape and avoid very specifi patterns (like cross, star) as they will always miss some data.
		- When center sample is available, decrease search radius only to get some smoothing - we don't really need to search for anything.
		- Or if center sample is available, we should give higher weight to samples similar to it?
		  With no modification, sharp shadows get to blurred when cover in soft shadow.
		- Or central sample should have very high weight and then we add other samples?
		- be carefull, because if I take many fa samples they will have bigger impact on final result. But they have decreased impact because they are further in world space.
		- KEEP ALL INTERNAL PARAMS SMOOTH - a bigger smooth erorr is better than smaller binary error.
		---------------------------------------------------------------------

		IMPROVE SAMPLING PATTERN FOR ILLUMINATION - as for dist-to-occluder

		- dist-to-camera should not impact the number of samples taken
		  It can impact their distance from center pixel, but  should not increase their number.
		  The reason is that when we zoom in, things on screen get bigger, so we don't need to take more samples to keep the same quality.
		  We only need to spread them on larger area.
		  * On the other hand we may need more quality when zooming in?

		- or we can simply make non-linear depandancy of number of samples taken from search-radius
		  * For large search-radius we are probably zoomed in, so not that many more samples are needed.
		  * But when we are zoomed out, we need very few samples.

		TO TEST:
		- assymetrical weighting of samples based on dist-to-occluder
			* sth like if center sample is sharp, decrease weight of blurred samples
			* if center sample is blurred, treat all samples equally (or opposite)
		- dividing illumination into two textures (sharp and blurred) and then sample a bit from each during blurring.

		#TODO: Check PowerVR method for rejecting sampled based on derivative.


		- why blurred-illumination is float4 instead of float? Could be good for future colored shadows, but now?

		Spreading/blurring blur-radius:
		- CAN'T MIPMAP because:
			* that leaks blur-radius from background to foreground (and opposite)

		- CAN'T SPREAD AT FULL RES because:
			* bad performance?

		MAYBE GOOD IDEA:
		- spread shadow only to surfaces further from light source than shadowed surface (or slightly closer). Should be easy to calculate...
		  May be a better check than comparing positions.. Accounts for light position.
		  But still doesn't solve blur from mipmapping.

		DESIRED SOLUTION:
		- keep mipmapping so that we can work on low-res texture and get some free blur
		- know when to use full-res blur-radius and when to use mipmap. How to know that?

		How to know that something should not receive any shadow from sorrounding pixels? Should I compare distance to light source more than position itself?
		Shadow cannot be spread on objects closer to light than shadowed surface, right? But we also store nearest distance, which could cause problem. We would also need farthest distance...

		CURRENT ISSUES:
		- how to blur something, while preserving edges? Use 9-tap filtering with position checks... Accept only samples with similar position to central pixel.
		  Regarding blur-radius - these 9-tap could be farther away from each other to simulate more blurring without increasing number of taps.
		  This would be the same approach as in reflections... Could work..
		  * But why did it work before? On screenshots? Did I do some check during blurring to check if samples are ok?
		    Maybe all shadow samples were rejected anyways, so blur radius didn't matter that much? But it still sucks to have it this way... Could cause problems anytime..
			No.. position threshold is not enough to stop this problem. It has to be solved properly...

		- IDEA: Do some spreading on full-res image first - like 4 pixels etc.
		  This should ensure that there are samples available at both sides of edges. Should spread correctly from there..
		  Nooo... not gonna help if we create a mipmap from this.

		- when we have available blur-radius in max-resolution - let's use it instead of mipmaps - should help with blur-radius leaks and improve quality
			* I could generate a final, full-res blur-radius texture for better debug - but it's not needed - the 'if' can be in the blur-shadows shader.
			* NOOOO... We can't sample from full res, because we don't know when to use it. Pixel which are lit have max blur radius anyways...
			* Hard to do, because mipmaps have samples with 1000 blur-radius. And when sampling that linearly we get very high values near egdes.
			* other idea: When deciding which sample to use - sample position mipmap level 3 and 0 and compare - if there are bigg differences - use 0 mipamp as there is egde nearby
		- blur radius-spreading from foreground to background and opposite - simpler to fix
			* fix mipmap generation for blur-radius to be aware of surface position
				> do I have to use pixel-size-in-world-space and camera-pos for this? Can we live with simple positon threshold?
				> Nooo, not easy, beceause mipmapping cannot use position as threshold - which pixel's position would have priority there?
			* fix spreading awarness of position - check calculations and if they even work..
		- blur radius-spreading ugly and with low performance - incorrect limiting of spread of low values
		- using sharp shadow samples when in soft shadow - have to fix prevois solution which required some normalization

		------------------

		- why high blur radius from front surface of flowers spreads to the wall? Didn't I limit it somehow using position threshold?
		So we have to limit spreding using screen-space limits and world-space limits?
		Maybe I should stick to world-space then, reading position texture and avoiding spreading blur radius through object egdes?
		I don't know....

		- create a couple of test scenes for shadows? Like if we pass all of them we accept the solution?

		CONCLUSION: SO.. - take position in spread shader and compare it against blur-radius in world space to avoid spreading to far on the same surface or spreading
		from foreground to background.

		TO TEST: When there are values all around central sample - don't do anything - this should keep natural edges sharp...
		Or just sample during blur-shadow from higher res mipmap if available...

		How to disconnect spreading pace from number of passes? Should low blur-radius spread more slowly? Probably not, just not spread at all at some point and be ready for some blending...

		- keep blur-radius in screen space until blur-shadows. 
		  Before that run a conversion shader which recalculates screen-space-brlur-radius into world-space-blur-radius.
		  Keeping them in screen-space facilitates rejecting low-blur-radius samples during spreading.

		- limit spreading blur radius for low values. One black patch shouldn't spread to the rest of the screen...
		  Discard samples which blur-radius is smaller than spreadDist (or sampling dist from center) in world space (or convert both to screen space?).

		- first spread in radius of 4px
		- then run compute every 4th pixel and spread once
		- then run every 8th pixel and spread, then every 16th, 32nd, 64th, 128th, 256th, 512th. -> Logarithmic complexity.
		  But better than mipmaping because we don't loose detail on edges. 
		  Plus we can't really mipmap blur radius, because we have holes in the data. (except for custom mipmapping with ifs).
		  Should also work fine, because far-reaching shadows are usueally soft and have little detail.
		- then run every 256th, 128th, 64th, 32nd, 16th, 8th? again - to fill the gaps.
		- in the end run every 4th, but with offset of (2,2) - that will fill pixels between our 4-spaced pixels.
		- then run every 2nd, and again every 2nd with offset (1,1). It's done.
		- When filling gaps, ignore neighbor samples which have value lower than dist to neighbor (passed as param to shader and depanding on the pass).
		- When more than one sample is available - take avarage.

		- BUG: When sampling during spreading blur-radius texture I accept samples from outside of screen - because thay value of 0 - they get accepted.

		- SOME PROBLEM: Depending whether we look at close-up or from afar using hardcoded 4 px gives different results...

		- Best to do the above using screen-space blur radius - easier to reject far samples.
		  But then it could be recalculated to world-space in a separate pass - so that rejecting shadow samples is easier in "blur shadows" phase.
	  ---------------------------------------------------------------------------



	  NEW TODO:

	  SHADOWS: ----------------------------------------
	  - maybe i should sample original dist-to-occluder texture instead of the final-dist-to-occluder?
	  - or it's mipmap... Because my shadow softness ratio is affected by that and maybe it shouln't be.

	  FIX: Fix 4-pixel stripes in shadows by first  performing 4-pixel wide neighbor search-for-occluder dist. Separate pass. Then another pass can jump 4 pixels at a time or maybe use a lower mipmap?


	  - during dist-to-occluder-search: weighting samples similar to center higher improves quality when shadows overlap (and transparent shadow over hard shadow etc).
	  But good weighting equation is needed and it probably should be one way -> only soft samples are weighted down when center is hard...

	  REFLECTIONS: -----------------------------


	  - add limting FPS when app is not in focus - already added a member for that


	  ------------- VERY, VERY IMPORTANT !!!!      ------------------------

	  - maybe sum weights related with distance separately and than square this weight or use smoothstep to ensure more smooth transition from near to far distances.

	  - We don't need to search hit-distance over the whole screen.
	  - That's actually incorrect as some objects would impact perceived roughness on completaly unrelated objects...
	  - The only thing we need is to balance transition between objects/sky and near object/far object on their edges.
	  - With some tweaking - it can be done.
	  - One way is to clamp weights for sky (far samples) samples - and limit them to 0.05 or so...
	  - Make "sort of Gaussian blur" when weighting normal samples - so that they fade further from center and blend smoothly with sky.
	  - Account for distance when calculating search radius?
	  - Can be optimized using two pass blur? Maybe, maybe later? Needed?

	  - How much generating mipmap matters? Rejecting samples? Having holes in mipmaps? Do we fill these holes when generating mipmaps?




	  ----------------------------------------------------------------------






	  - IMPORTANT: Maybe very important - clamp hit-dist values before creating mipmaps. Otherwise values interpolated on textures will be too high...

	  - play with hit-dist weighting based on screen-space diff.
	  - account for distance to camera in hit-dist-search - should impact search area

	  - MINOR: Center texcoord doesn't really point to pixel center on input texture. It points to center in output texture. This can cause problems.

	  - DOESN'T HELP MUCH: during hit-dist search can sample between mipmaps... To smooth things out

	  - use thresholds, rejecting samples in combining shader as in hit-dist-search - otherwise the glow will appear

	  - add some Settings class to store settings (const and non-const) for Rendering algorithms.

	  - Perform hit-distance search at half-res or quarter-res - should be fine and even improve quality

	  - can we avoid sampling the same regions when sampling higher mipmaps? Because we sample the center area a couple of times... Or maybe that's fine?
	  - do we need position and normal check in hit-dist search? Maybe we need them more for higher levels of reflection/refraction.

	  - hit-distance search radius should depend on pixel size in world-space (depth) etc... Otherwise avaraged hit-distance changes when zooming-in or out. 
	  - reenable position check when taking samples from reflection image - could also be a reason for glow around objects
	  
	  
	  - add multipliers to textures... But may restrict some multipliers to 1, because some textures cannot emmit light.. And for some textures it makes no sense to have a multiplier?

	  Sources: 

	  Another idea for shadows with quality of full ray tracing:
	  https://www.cs.purdue.edu/cgvlab/papers/popescu/popescuGEARSCGF2014.pdf

	  - profiling, GPU queries: http://www.reedbeta.com/blog/2011/10/12/gpu-profiling-101/

	  - calculating reflected ray direction: http://graphics.stanford.edu/courses/cs148-10-summer/docs/2006--degreve--reflection_refraction.pdf