19.03.2014
	- when creating fipImage in Texture2D - FREE_IMAGE_TYPE is set to FIT_BITMAP by default. However, it is not always the appropriate file type for image. Ex.: 16 bit grayscale heighmaps have different file type. Maybe it should depend on special file formats defined in AssetFileFormat such as RAW_16BIT?
28.03.2014
	- Handle errors in AssetManager::loadAssetsFromDisk and AssetManager::loadAssets - threads shoould not be terminated, exceptions should not be thrown, path to assets should be removed from assets - maybe handle by not doing anything?
11.04.2014
	- Write = operator for every class with copy constructor (otherwise default copy constructor will be used)
12.04.2014
	- check whether all DirectX headers point to the right SDK (by pointing the cursor over an include)\
	- add loading to Gpu to asset manager (read about async resource loading before - probably one loading thread is enough)
08.11.2014
	- add more methods for manipulation float44 and float43 - ex. getRotation rename to getRotation, get4x3 rename to getRotationTranslation, add setRotationRow1 etc
	- change bone pose format from float44 to float43 - beacuse last column is useless anyway
15.11.2014
	- methods which return references (especially to lists) should retrurn shared pointer rather than reference. 
	- add File class with path, type, data, loadedFlag? use it in all assets classes (Mesh, Texture etc)
	- SkeletonAnimation requires bonesNames to work (mapping of names to bone indexes) - how should these names be passed so that each SkeletonAnimation doesn't store a copy of that list (shared_ptr?)
	- should boneNames have it's own class with methods like getBoneIndex( std::string name ) ? - would it be used anywhere else but loading animations?
21.11.2014
	- does combining two SkeletonPoses really work? Especially in the case where the same bones are manipulated in both poses. How about joining three or more poses?
	- optimizing SkeletonPose in terms of memory usage? What is the size of this object when used for a typical or large animation?
28.12.2014
	- Const methods ( ex. const Mesh & getMesh() const ) should always return const-object or const-reference. Non-const versions of the same methods can be created to allow modification of the object. 
	- add const and non-const version of some methods (getters mostly) to all classes. Use const method in non-const method. Use const_cast<type> to convert types from const to non-const.
13.07.2015
	- Go through all "throw std::exception" and make sure that throwing method has parenthesis "()" in the error message.
	- Add tests for SkeletonMeshVertexShader checking if it throws for incorrect bones-per-vertex-values.
11.11.2015
	-  Texture2D::createFromMemory - WARNING: casting away const qualifier on data - need to make sure that FreeImage doesn't modify the input data!
12.11.2015
	- AssetManager: add due time as priority for each asset in AssetManager. Asset's subassets should be loaded with it's owner's priority.
	  When inserting assets to load in the queues they need to be inserted in the order of increasing due time.
	- IMPORTANT: There is one problem: if one thread (loadAssets method) waits for the sub-assets to be loaded, then it's not loading any assets at that time. Maybe there should be more threads, but only the given number of them active? Non-binary mutex or something?
29.04.2016
	- [Texture] Add constructor from exisitng DX texture and render target view (or swap chain?). For access to the backbuffer.
	- [Texture - Refactor] Can I remove initialize methods from Texture2DGeneric? Move that code to constuctors...
	- [Includes Cleanup] Some headers include <d3d11.h> although they only contain references and pointers to DX types. Replace with forward declarations.
	- [Test Texture] I should test whether constructor options like generateMipMaps (ex. for RenderTarget) really work for all the bindings and remove them if they are not supported.
21.08.2016
	- [Texture - new] How to handle texture arrays (also with mipmaps). Should it be a separate class. How to handle all that complexity? How to use the existing texture class?
	- [Shader params] Add support for setting empty SRVs, UAVs etc among non-empty ones.
31.08.2016
	- [ Texture copy ] When copying texture GPU - GPU we should check if their dimensions are the same - otherwise throw exception.
20.09.2016
	- [ BinaryFile ] Reading/Writing could a template method - able to read any type from file - such as int/float/float3 - it works as a copy anyways...
25.01.2016
	- [RendererCore - refactoring] RendererCore::copyTexture method is a template, so any texture works fine when passed. 
	  But compiler cannot deduce template parameter type and perform implicit type conversions at the same type, so it's confused and fails to compile.
	  How to avoid having to explicitly cast textures to avoid compiler confusion? Add some methods for the most common texture types and use template version inside?
03.02.2017
	- [ Path ] Replace all std::string paths with std::filesystem::path - it's internally stored as std::wstring, but allows for many useful operations - such as extracting filename etc.
07.03.2017
	- [ Optimization ] Disabling rendering/compute pipelines could be easily delayed until a different pipeline is needed. Simply remove "disable" method and disable when enabling (if needed).
	  What problems could it cause? Some problems when configuring pipelines? I could enfore that a given pipeline type is enabled by exceptions.
29.10.17
    - [Shaders] Rename all constant values to start with "g_" prefix.

For later ######################################################################################################################

//////////
 - Add some math functions to measure corruption of matrices

 - Add: matrix difference (sum of squared distances), matrix inverse, measure matrix orgonality (multiply inverse by the transpose and return difference to identity matrix)
 - Optionally add: orthogonalization - later on... To fix corrupted matrices and compare visual results

/////////

- option to read os some range of keyframes/time/ticks from animation file
- option to split an animation into a few by keyframe/time/tick etc.

- add move constructor to some classes? (but only the ones which are designed to be returned from methods by object) 
- add move assignment to some classes? (but only to those for which creating another object from an existing one makes sense)

- test combining poses (skeleton-space, parent-space)
- add method to substract poses (leave only the difference between poses)

- add option to ignore bones which are not in mesh when loading an animation file. How to make sure that missing bones are not in between of the existing ones? Useful to apply complex animation to a sipler version of mesh (LOD etc)

- Refactor Font and FontCharacter classes.

- in BlockMesh, SkeletonMesh buffer getters should return reference?

- 3d text rendering

- make RectangleMesh derive from BlockMesh (to be able to render it with BlockMeshShader) ?

--------------------------------------
	Texture2D class:

	- No generic class without template params to be used as method arguments.
	- PixelType has no usage for textures which are only stored on gpu (like render target) - but restricts their usage in methods etc.

	How to deal with textures which are not loaded, but store file information (when parsing ModelTexture files etc). 
	Should it ba possible to create empty textures? But lock them internally to make any action impossible?
	But probably a more elegant solution can be found. 

	**** IMPORTANT 3:
	- maybe there is no point to add compile time checks related to texture usage? Or maybe we can simplify it somehow? Because we use only some subsets of all congurations.
	Like when binding is RT or UA then it's obvious that texture is not Immutable - probably Default usage.
	Also there seems to be no point ic creating a texture which is only RT, but not UA. Or RT but not SR - you usually want to read what you just rendered there...
	Maybe just a couple of "texture types" would be enough? Like RenderTarget (Default, RT_UA_SR), StaticTexture (Immutable, SR), DynamicTexture (Dynamic, SR, maybe RT_UA?), Staging.
	But need to make sure that asking for SR texture works fine - without some stupid casting etc..

-------------------------------------
	IMPORTANT: AssetManager tests (event the first one) make whole test execution to fail for all tests. But only in Release builds. It works fine in Debug. Why?
-------------------------------------

##################################################################################################################################

 - IMPORTANT: ray tracing shader has better performance with D3D10_SHADER_SKIP_OPTIMIZATION flag set. Why? 
   How to configure the shader to avoid optimizations only where it really improves performance, but not everywhere?
   
   Also2: pass stack depth to the shader as constant or...
   Also3: Compile a few versions of the shader with different stack depth and use the appropriate one?

	 - textures which use float4 for vectors (rayDir, rayOrigin etc) could use float3, because such format exists.

	- PERFORMANCE: buffer containing "surface position" in DeferredRenderer could be reused to store ray origins in the RaytraceRenderer. 
	    It would have to be read and rewritten as UAV to slithly adjust ray origins along the surface normal to avoid self-collisions of ray with the reflective surface.

	- check how reflection looks when calculated only for even pixels. middle pixels can receive half of the calculated color from each side (addition).
	  What's the performance difference?

	- algorithm creating BVH uses the same step when moving dividing planes no matter the size of the model. 
	That's not too goood. It should use something like 1/1000 of longest bbox dimension or similar.

   ----------------------------------------------------
   Refactoring:

	- use short "Idx" for "index" in the whole code + shaders.

	- add uint type, which would be just a typedef for unsigned int - could be in math in separate header file.
	- rename "ReflectedRefracted..." shader names to "SecondaryRays...".

	- shader could use g_ in global variable names. Because names often reapeat.

	- get rid of "Direct3D" from class names. It's used all over the code anyway - no reason to mark specific lass with such prefix.

	- rendererCore should also allow for setting viewport dimensions and depth range. It's done manually in FrameRenderer and CombiningRenderer, but it's wrong this way.
   ---------------------------------------------------

*** IMPORTANT:
It's strange that we set textures in FragmentShader's setParameters method, but we set buffers in RendererCore draw method. It should be unified.
Example: draw(or renamed to render) method should take model instead of mesh and set textures accordingly. Or buffers should be set in VertexShader setParameters method...
Also constant buffer is set in setParameters...
Also only the shader know on which slots it binds resources. So only the shader know ho to bind/unbind them correctly. Doing it from outside is just guessing and won't work for more complex/diverse shaders.
So probably set buffers, textures, uavs from shader class.

**** IMPORTANT 2:
Maybe I should createa Buffer class. Same as Texture2D. Because we need to store quite many resource ptr to directx with each buffer plus C++ vector. Could be good to group them up. 
Could be a template class taking it's element type at template argument. It would also simplify sending data to and from GPU using buffers. When building volume data structures etc.
+ Check bind flags on different resources - ex. vertex buffers (and normals etc) have bind flag - "default", but should rather hvae "immutable". Default means that I expect GPU to modify these buffers.



Idea:
- when calculatin range of a light source - it's bounding volume doesn't have to be a sphere. The more precise the bounding volume to more calculations we save. Maybe even a few volumes could describe a lights range?

----------------------------

	  - should I rename refration into transmission? To differentiate them more..

	  - if we hit surface from inside (deduced from normal), than we need to invert refraction index? How to know refractive index of air, water if we are in one mesh, inside another and another etc?

	  - rename albedo render targets to albedoAlpha.

	  - rename reflectionTerm texture in combining shader and classes. Should be called contributionTerm. Check for incorrect assumption regarding reflection/refraction too.

	  - experiment with increasing minimal contribtuion for the ray to be generated and traced. How high can we go for higher levels?

	  - add exceptions and check if wrong level is passed to reflectionrefraction renderer.
	  - remove unnecessary 'level' and 'prev contribution term' from firstreflectionshadindg and firstrefractionshading.
	  
	  -----------------------------

	  

	  - support for any resolution. - almost done. Something wrong with higher level reflections/refractions. Maybe the contribution term fill size etc? And there is no refractions...
	  - reflection/refraction shading renderers also have some buffers which should have lower resolution. Account for that and downsize them to the same size as raytracer's buffers.
	  - should reflection/refraction shading use integer texcoords? As it's input the same size as output always?

	  - suport for any resolution - part 2 - lower resolution should be accounted for correctly in combining shader to avoid increasing roughness globally (as it happens right now).

	  

	  - how to make skybox not receive shadow? To improve performance...



	  /////////////////////

	  OPTIMIZATION: Calculate pixel position from depth - at least for first reflection/refraction... How about further levels?

	  IDEA: Geometrically incorrect shadow:

	  We create soft shadow within the hard shadow. This way we always know the blur radius. Problem: soft shadow at the edge of hard shadow is half dim. We have to account for that later on.

	  I should also probably use point sampling in bluring phase? Otherwise there is always some mixing of soft/hard shadows for near samples. Even if I reject some of them...

	  Add rejecting samples with wrong normal or depth? Both or just one criteria?

	  I can increase shadow area (to correct soft shadows going only to the inside) by modifying fov, shadow ray direction etc... So it doesn't have to be completaly fake... :)

	  Making edges brighter will fail for shadows from semi-transparent surfaces... How to handle that?

	  //////////////////

	  CURRENT SHADOW PROBLEMS:

	  //////////////////

	  - check which part os lambo has problems? Test1: make everything white, disable shadows and check if shading is ok for everything.

	  

	  - REFERENCES: Screen space soft shadows - described in GPU Pro 6 !!!!!!!!!! Check it!!! 

	  - there should be a second version of RasterizeShadows shader, which takes prev layer ray origin as camera position in each pixel. To handle correct blur radius in reflections.

	  - IMPORTANT MAJOR FIX: I could fix most of the near-screen-edge problems by rendering the whole thing with a bit larger resolution... 
	    I could even lower the quality a bit for those regions. A lot of possible ways and not too much of overhead... I could also ignore those extra regions when performing blurs etc.


	  - OPTIMIZATION: When I read from raw buffers in raytracing shaders I can read multiple values at once. Using Load2, Load3, Load4 methods... I need to try it!!

	  - check all shader with SKIP_OPTIMIZATION flags and check if they are faster/slower with/without this flag.

	  - VERY IMPORTANT: should I switch to reconstructing position from depth globally - it would reduce read bandwidth by factor of 4... And it seems to be the bottleneck...
	    But I need to know ray direction anyways...
	    But maybe later once I clean up the mess with shadows...

	  - why shadowRaytracing shader costs 2ms even if it does nothing, but color the regions for which rays will be traced... Because of running it for every mesh... How ti improve that?
	    I am probably burning bandwidth like hell by reading the same textures (screen-size) over and over..
		Or maybe the problem is the number of runs or the fact that the shader writes to the illumination buffer... Maybe it shouldn't write if does nothing...
		- Idea: I could handle the first copying from preillumination texture as a separate pass... to avoid worrying about it later on for every run.
		- Better idea: Just copying values from preillumination texture. Or preillumination could be copied entirely using a copy drawcall (no need for another shader).
		- Reading the "ray origin" texture seems to be the bottleneck. Probably because it's float4 expensive format. I could use depth instead to save some bandwidth. But it's still not scalable enough...

	  - OPTIMIZATION: Can I deduce for which screen region it is worth to run shadow raytracing shaders by projecting mesh's bounding box on screen - to avoid running shader for pixel which won't do anything in that particular run.
	    To disable whole groups of threads by running them only in some square region of screen.

	  - OPTIMIZATION: when detecting pixels for which its useful to trace shadow rays I detect the need for pixels at the edge of spot light cone. There is no point to trace these rays. How to avoid that? Calculate dot in the shader for that light and compare with cone angle?

	  - PROFILING: how to track the total number of traced rays?

	  - what is the size of the penumbra? Source: https://www.nde-ed.org/GeneralResources/Formula/RTFormula/Unsharpness/GeometricUnsharpness.htm
	    How much to blur shadows based on light source size, distance from light ot occluder and distance from occluder to shadow receiver.
		u = lightRadius * ( occluderToSurfaceDist / lightToOccluderDist)

	  - Good paper (a bit old now...) describing various soft-shadow techniques: http://maverick.inria.fr/Publications/2003/HLHS03a/SurveyRTSoftShadows.pdf

	  - use sampler2DShadow as in http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/ to know if we are on the shadow border...?

	  - optimization to shadow mapping - use border value and D3D11_TEXTURE_ADDRESS_BORDER and remove if in the shader checking if sample is within texture area.

	  - display GPU name and optionally also CPU name on the screen

	  - check which shaders are running with "skip optimization" flag? Maybe we slow down because of that... Is raytracing shader still faster with that flag?

	  LATER:

	  - all file parsers should:
		- append data to given buffer when writing (not clearing it as it is now!)
		- read data while checking not to cross the given end iterator - should also check for negative sizes etc.
		- how to elegantly avoid resizing data vectors during write (ex: mesh + bvh). - probably the same way as in BVHTreeParser

	  - do I have to account for window frame height, borders etc when creating window? Is my image streched down?

	  - OPTIMIZATION: support for variable resolution per reflection/refraction depth

	  - OPTIMIZATION: Trace 4 or 8 rays in a single shader. Could give amazing boost, because we construct triangle only once and do a few intersection tests with it. Could be tested in normal raytracing or shadow ray tracing (even soft shadows).

	  - OPTIMIZATION: Mapping/Unmapping constant buffer each time we setParameters to shaders has to be very costly. How can I avoid that?

	  - cubemap texture class - to use for skybox

      - whenever i have an explicit copy constructor, I need to call a base class copy constructor directly - add where missing.


      - modify AssetManager so it could load many assets from the same file at once - should there be an option to load just one on many?
      - should I collect the info about materials somewhere? Like additional data structure? That would have to be stored in AssetManager...dirty
      - maybe an ability to load .obj files as BlockModel? Then it would read needed info at once without stroign it?

      - make shader debugging work in VS 2017 - debug combining shader to check why the new idea with expected position-difference doesn't work.
      - probably need to use Dependency Walker to deduce which library or pieco fo code uses VS13 runtime dlls
      

      In combining shader:
      // #TODO: Maybe I should calculate the expected difference in positions between center and sample. 
     // And then use ratio of positon-diff to expected-position-diff as weight?
     // Because right now we dim outer pixels just because they are naturally further away. 
     // And it's fine, but it's mixed with filtering pixel in the background.
     // These two processes should be clearly separated and tweakable. So two kinds of weights calculated...

      - sampling artefacts show in stripes because we sample color at 4-th mipmap and position/normal at 0 mipmap. So weight assigned to sample changes over the samples area...
        Stripes are more visible than normal erorrs - I should fix that somehow..
        Should I sample a higher mipmap of position/normal? That could also lead to artefacts as position on the edge would be a mix of two positions...
        INTERESTING detail: At the outer part of each stripe the sample is correct - no influence of wrong reflection. So Maybe I can sample pixel at corners to deduce it's weight?
        And use the lowest weight - or simply use the highest position difference...?
        Or I could make that center pos enough to reject that pixel... But how?
        IMPORTANT: But I should still fix this as this error appears also for correct samples - as these stripes are visible...


	  - my hack for skybox - "don't raytrace if roughness is maximal" causes problems in the mirrors and some other objects - what other trick can i use?

	  - improved roughness handling helper a lot - but it also caused black lines near some rough edges - because correct samples cannot be found.
	    I should handle such case using if - when samples were not found - and simply use central sample then with some less agressive blur.

	  - make sure to sample buffers using CLAMP sampler - whenever i do some blur etc... To avoid reading the other side of the screen

	  - remove m_imageHeight, m_imageWidth from most renderers? Pass that data in rendering calls instead or deduce it from render targets?
	    Or deduce it from targetMipmapLevel - as we don't allow for arbitrary image dimensions - rather some multiplies of screen dimensions..

	  - render mode where everything is white/gray - for shadow testing - could be done by forcing default albedo on objects..

	  - refractive index may be working incorrectly - we don't set it up yet in Renderer and it's not set in RaytraceRenderer anymore.
	    Especially things like currentRefractiveIndex are probably wrong. Check with some glass spheres etc...

	  - mipmapping for raytracing - use UV texcoords stored in g-buffer?
	    Or use ray differentials - http://graphics.stanford.edu/papers/trd/
		Right now a funny hack is used to get "some" filtering - log2(hitDistance) is used - but it's stupid and if you have an object far away it will use 1x1 mipamp for sure....

	  - add debug view before bloom, tonemapping? To see if a problem comes from which postprocess.

	  - accumulate distance to camera along rays for any number of reflection/refraction layers? To calculate blur radius properly.

      NEXT:
      - look like combining2 measures position error differently than combining1 - not using expected possition diff. Fix that! Or maybe it isn't used in the end...
      - make sure that recent changes in cominign shader were also added to comining2 shader. (Gassian scaling of samples etc, sampling quality).
      - is specular reflection (as in shading) growing to the bottom onyl, or also to the top? Also to the top, we could improve that - elongate reflection in one direction mostly.
      - PROBABLY DONE: reflections/refractions are never fully sharp - it's because of combinign shader which always does some blurring - fix it!
      - add reflection elongation based on surface normal in screen space (should elongate along that normal, based on it's length in screen-space)
        What is the relation between specular elongation and roughness? Rougher surfaces have lower elongation? And perfect mirrors have zero elongation?
      - move all shader setup thresholds to settings - and add them to Control Panel
      - reflectivity texture and multipliers
      - I think that emissive color is clamped to 0-255 as it is stored in uchar4. It limits the intensity of emission. Should we increase its precision? Change it to floats? Quite wasteful.. Or maybe apply color multiplier later on - per object...
      

	  IMPORTANT-BUT-CAN-WAIT:
      - OPTIMIZATION: Replace float4 with float3 wherever possible in framebuffers/shaders.
      - OPTIMIZATION: make sure to check for max ray distance when intersecting rays with Bounding boxes in view raytracing. As it's done in shadows...
      - getting mouse pos is bugged - get absolute positon, while should get position relative to window.
      - add versioning of files? How to add that for files without versions?
      - I can remove this index-in-file, format etc from file-infos...
      - picking objects does not work correctly if window is moved somewhere from top-left corner...
	  - how to deal with light sources placed inside geometry? It causes artifacts now..
	  - most shaders are configured for that specific resolution - blur kernels etc should be scaled with resolution...
	  - improve bloom to use two-pass blur and to use less mipmapping...
	  - input to antialiasing FXAA is probably not correct- linear vs non-linear, gamma etc... So antialiasing doesn't work optimally...
	  - account for ray-length in next hit-dist-search?
	  - account for depth in hit-dist-search
	  - account for roughness in hit-dist-search? Otherwise setting very rough surfaces doesn't give enough effect as edges are too sharp..
	  - shaders that have several quality options, which we want to change only for test could be compiled for each option - then at runtime correct shader could be loaded based on settings...
	  - make sure that useless rays are not traced...
	  - almost for sure contributionRoughness has to be rethought - do we still need to accumulate values? Maybe... (to avoid tracing some rays), but should we use that when combining? or another buffer which only has curr layer contriburion and roughness?
	  - remove rejecting rays based on contribution? Does it even help in real cases? How much screen space do we save by using that information? Mark red non-traced rays from contribution?

	  - Distance-to-camera in RaytraceRenderer doesn't account for initial depth (which should be converted to dist-to-camera).
        Another shader should be used to calculate dist-to-camera from depth after deferred rendering.
        * Does this summing work correctly when both refraction/reflections are enabled (but only one dit-to-camera per layer, right?)?

	  NEXT-MINOR:
      - button to reload textures for all models - useful if I change textures on disk, but don't want to reload the scene, loose selection etc.
      - add saving/loading light attenuation multipliers to file
      - rendering text should be moved to a separate renderer - TextRenderer. No point having it in deffered as it's not outputting to most render targets. Only to albedo...
	  - problem with white squares in reflections - caused by very small, bright highlights - thay occupy almost only one pixel and spread using bloom weirdly? Should image be blurred before bloom?
	  - setting default texture from control panel - to metalness, roughness (defaults should be maxed out).
	  - select all lights
	  - debug view with only lighting visible (from all lights) - but all surfaces are gray, non-metallic, without reflections.
	  - colored shadows
	  - improve sampling shape in all shaders by taking samples in spherical shape - 
	    simply exclude samples which are too far from center (box corners) - this could even improve performance, by reading less pixels.
	  - scale blur kernels based on view-angle assymetrically - horizontally and vertically - ex. for walls we should flatten horizontally and for floors vertically. Assuming that camera is not rotated around view-dir.
      - should all file-infos have filename instead of path? Probably not... - but it's misleading if the full path will be used or just a filename...
      - Texture2DFileInfo::PixelType should be probably moved to the texture class itself - pixel-type is a general property of a texture, not just a texture file
      - merging algorithm should probably be able to stretch-up/rescale textures to much some placements... (maybe if given some flag, max stretch-up-down ratios etc)
      - when calculating how much memory is used in RenderTargetManager - it would be best to be able to read the maximal memory used during a frame - including the short spikes - that would require recalculating that value everytime a texture is acquired and resetting it at some point.
	  - I could implement a GPU path tracer to be able to compare normal renderer with physically perfect renderer. That could help a lot in my pursuit of great quality.
      - rendering mode showing showing each object with different, random color. To see where different objects are.


      //////////////////////////////// REFLECTIONS/ REFRACTIONS //////////////////////////////////////
	  ////////////////////////////////////////////////////////////////////////////////////////////////

	  - NOTES REGARDING ARTEFACTS IN REFLECTIONS NEAR OBJECT EDGES:
		- the artifacts come from two sources: accepting samples which are too different from the center one (in terms of positions/normals etc)
		  and from avaraging of colors caused by using higher mipmaps - even in the center pixel.
		- artefacts got fixed by rejecting samples based on position/normal and by rejecting center pixel automatically.
		- the only visible artefacts right now are from too high specular highlight in reflections - fixing specular should fix that.
		- To fight that we can:
		  1) use smaller mipmap levels (and more samples) - that would reduce color bleeding from mipmapping (used now), expensive, but simple
		  2) rejecting samples in a smarter way, knowing somehow if they contain artefacts from mipmapping (probably some radius from center can be calculated) 
		  3) generating mipmaps for positions/normals and sampling them at the same level (with the same sampler) as color to deduce if they contain artefacts from mipmapping (is it expensive?)
		  4) Spread our samples more from the center (than exaclty the size of the pixel) - thus reducing mipmap artefacts - but may cause an effect of duplication of reflections... worth a try.. 

	  - NOTES REGARDING HIT-DIST-SEARCH for reflections:
	  	- We don't need to search hit-distance over the whole screen.
		- That's actually incorrect as some objects would impact perceived roughness on completaly unrelated objects...
		- The only thing we need is to balance transition between objects/sky and near object/far object on their edges.
		- One way is to clamp weights for sky (far samples) samples - and limit them to 0.05 or so instead of giving them 0 weight (which requires to get some value from elsewhere in the screen)
		- Make "sort of Gaussian blur" when weighting normal samples - so that they fade further from center and blend smoothly with sky.
		- How much generating mipmap matters? Rejecting samples? Having holes in mipmaps? Do we fill these holes when generating mipmaps?
		  How eraly should we clamp distance values? Seems like early clamping is a good idea.
		  (My guess: clamp hit-dist values before creating mipmaps. Otherwise values interpolated on textures will be too high...)

		- #TODO: Find good pos-mul, normal-mul and threshold for rejecting samples.
		- #TODO: Should account for distance when calculating search radius - further regions blur smaller screen areas
		- #TODO: Can be optimized using two pass blur? Maybe, maybe later? Needed?
		- #TODO: Doing reflections in half-resolution? Especially raytracing?

		CURRENT PROBLEMS:
		- I use more samples with lower level mipamps to avoid mipmapping artefacts (colors blurred over object edges) - but it's expensive - maybe could be improved.
		- small values of hit-distance get too blurred with the sky (far samples) - and reflection is too blurred when object touches the reflective surface... 
		  Maybe weight sky depending on the original hit-dist (before blur)? - DONE - Improves, but not perfect. Works within the object, but not on the outside.
		  Could look for min hit-dist in sorrounding pixels before deciding on how to weight sky (far) samples.

      DIFFUSE REFLECTIONS USING MULTIPLE RAYS
      - if we cast more rays we need larger g-buffers - that limits how many rays we can cast
      - but if split screen into fragments and calculate one fragment at a time - we can store all rays in one image
      - example: fragment is 1/4th of a screen, so we can store 16 rays at normal screen g-buffer and calculate them all at once.
      - the bigger problem is how to calculate higher layer's ray origin/direction? Because we now have many rays, not just one...
      - maybe we can keep the resolution and trace 1/4th of rays at next layer - we assume it's very diffuse reflection anyways...
        This way ray count stays the same, but we get approximate incoming light.
      - we would probably still need to trace specular reflections as a special fragment or something...


      - PERFORMANCE IDEA: higher level reflections/refractions could be calculated with smaller work groups - because only some pixels in the group need to be traced and many are black. Large groups cause waste of cores.

	  - MINOR: do we really care about solid object having reflections from backfacing triangles? Like inside a glass wall or aquarium wall? How could we ignore it elegantly with customization?

	  - should alpha impact the surface color used for shading? Yes. Should it also impact shading reflections?

	  - NEXT: dealing with varying refraction index.
	  - when we enter a surface, we read refractionIndex from screen buffer.
	  - when we leave a surface, we read refractionIndex from our last refraction index (stored in the buffer?)
		- maybe use texture array with refraction indices (like a stack) and the other texture as a counter of where we are on that stack for each ray.
		- when entering a surface - push to stack, when leaving - pop from stack (by decreasing the counter).
		- what format to use for refraction index
		- only need to access these textures when generating secondary rays.
		- store these textures in raytracer like other textures.
		- where to pass/store initial refractive index? Common for all primary rays.


      - OPTIMIZATION: I probably don't need float for blur-radius. A BYTE should be enough? We also don't care about fractional parts...

	  - IMPORTANT:  In generate refracted rays shaders - Why HLSL compiler crashes without "skip optimization" flag?

	  - PROBLEM: If we don't enter an object, we don't know what's the refractive index outside of that object... So if we start inside of a glass ball, we don't know which IOR to use when leaving the ball.

	  - IMPORTANT: Investigate whether we need to invert normals during normal shading (for backfacing trangles).

	  - IMPORTANT: Pass refraction index for current and prev object when shading refractions (in refraction shading shader).

	  - PERFORMANCE: Maybe it's faster and still correct to invert normals (when ray hits from backface) in raytracing shader rather than in each other shader separately.

	  IMPORTANT:

      - some problem with refraction - when seen in mirror it doesn't work? Or works at some angles only? Weird...
      Is it caused by incorrectly calculated contribution?

	  - we assume that refractions are always computed through shaders/classes with "fisrt" in their name. But that may not be true. And the name is misleading. 
	  Because it only happens like that if refraction occurs first, and then reflection/refraction. But if reflection is fisrt, then these shaders/classes are not used at all. What kind of bugs can result from this.
	  Rename shader and classes and replace word "first" with something else.


      ////////////////////////////////////////////////////////////////////////////////////////////////////////////
      ///////// Shadows //////////////////////////////////////////////////////////////////////////////////////////
          
      - what if I write dist-to-occluder slithly outside of given shadow layer? But only dist, no shadow value - to ensure smoother change of dist-to-occluder at transtion areas...
        Because right now it seems that light leaks are caused by different blur radius at the transition area rather then shadow values being too low..
        But we should do it only from softer layer to harder - not in both directions - because we take min from dist-to-occluder 
        and it's better if dist-to-occluder which belongs to a given layer overwrites to the one coming form another layer.

       - maybe a second dist-to-occluder-search pass could be useful - to smooth out the edges of the final texture - to avoid sharp transitions in case the spread was not sufficent.
         It could be run only on black pixels - to not corrupt the original results.

       - replace if's in the shader getWeightLowerThan() and similar with some math calculations.

       OPTIMIZATION:
       - sample-count-per-size in blur-shadow-shaders should not be fixed - it should be different for each layer, probably depending on radius etc.
       - blur-shadow stage should also be able to operate on different mipmap levels (output texture sizes) for hard/medium/soft shadows. 
         Or maybe not, as it may cause light halos around foreground objects with shadow in the background. Need to be checked.
         
       - can we split dist-to-occluder-search into horizontal and vertical pass? HOw many samples that would save?
         And what would result from doing two smaller passer (half radius etc) rather than one big?
         Performance seems to be fine for now - ignore.
       - raytracing shadows can be done at half resolution (or much less for reflections). And then sampled with interpolation.
       - OPTIMIZATION: I probably don't need float for illumination-blur-radius. A half should be enough? We also don't care about fractional parts...
       - IMPORTANT: Lights should be split into groups, within which lights don't overlap - and only one dist-to-occluder-search and blurring is needed for the whole group.

       Remaining issues: 
        - light leaks at layer transitions
        - to ensure smooth transition between shadow layers - dist-to-occluder has to change smoothly in the transition areas.
          Now it changes abruptly as it is taken only from one selected layer with hard threshold. How to do that? 
          Some blending of dist-to-occluder values when saving them to the texture, weithed depending on blur-radius?

        MAYBE GOOD IDEA:
		- spread shadow only to surfaces further from light source than shadowed surface (or slightly closer). Should be easy to calculate...
		  May be a better check than comparing positions.. Accounts for light position.
 		  How to know that something should not receive any shadow from sorrounding pixels? Should I compare distance to light source more than position itself?
		  Shadow cannot be spread on objects closer to light than shadowed surface, right? But we also store nearest distance, which could cause problem. We would also need farthest distance...

        - ideas to optimize blurring using shared memory: http://shiba.hpe.cn/jiaoyanzu/WULI/%E6%96%87%E4%BB%B6/soft/directX/Introduction_to_3D_Game_Programming_with_Directx_12.pdf
	    page 721

        DIFFERENCES COMPARED TO SSSS from GPU PRo 6 - they do blocker search as in PCSS and then save result to screen texture, and then blur in screen-space.
        Thay do some trick to avoid calculating each light separately - not important right now.
        I do everything in screen-space (and I have to because of ray tracing) - blocker search (dist-to-occcluder-search) and then blurring.
        This way I'm able to achieve unlimited resolution, unlike Shadow Mapping. But it's more tricky for me to work in screen-space all the time.
        I also have huuuuge penumbras compared to them (10 times larger), with near constant cost, proper mixing of hard/soft shadows 
        (they haven't showned that, has to work bad because of common blocker search phase)

      ////////////////////////////////////////////////////////////////////////////////////////////////////////////
      /////////  PHYSICS  ///////////////////////////////////////////////////////////////////////////////////////

	  - add some code to generate collision hull for selected object (or all of them?) and create it's physics?
	  - where to store actor's physics? Should actor has it's separate xform or take it from physics directly? What if ana ctor doesn't have physics?

      //////////////////////////////////////////////////////////////////////////////////////////////////////////
      ///////////// MERGING MODELS /////////////////////////////////////////////////////////////////////////////

      - stretching up textures to nearest power-of-two when merging

      - why textures are not layed out from top-left to bottom-right? Check that...

      - are merged textures saved upside-down compared to what is needed?
      - auto re-scale texture to nearest power-of-two when merging - prefer up-scaling than down-scaling.
      - additional check whether model texcoords are all zeros should be performed per input model, not per merged model -
        meaning that a single mesh should have its texcoords offset if all of its texcoords are zeros.

        //////////////////////////////////////////////////////////////////////////////////////////////////////////
        ////////// POST-PROCESS //////////////////////////////////////////////////////////////////////////////////

        BLOOM TODO:
		- split blur into 2-passes and increase number of taps
		- rename blur method and remove radius param.

        ///////////////////////////////////////////////////////////////////////////////////////////////////////////
        //////// TESTING / BENCHMARKING/ DEMO /////////////////////////////////////////////////////////////////////

        - Creating/storing keyframes for camera
        - Saving/loading camera paths to a file (text file is preferred to enable editing)
        - Saving/loading light path to a file (text file is preferred to enable editing)
        - Playback of camera path, smoothing rotations, movement
        
        - Benchmark tool:
        - number of repeated runs per setup
        - list of settings to test, scenes to test, camera paths, light paths
        - calculating average, max, min FPS, frame time
        - calculating accumulated total shadowing time, time per layer, reflection time etc.
        - repeating the same camera path for many settings 



	  -----------------------------

	  - MINOR: Center texcoord doesn't really point to pixel center on input texture. It points to center in output texture. This can cause problems.

	  Sources: 

	  Another idea for shadows with quality of full ray tracing:
	  https://www.cs.purdue.edu/cgvlab/papers/popescu/popescuGEARSCGF2014.pdf

	  - profiling, GPU queries: http://www.reedbeta.com/blog/2011/10/12/gpu-profiling-101/

	  - calculating reflected ray direction: http://graphics.stanford.edu/courses/cs148-10-summer/docs/2006--degreve--reflection_refraction.pdf